# 02. 딥러닝 기초 (Deep Learning Foundations)
# 신경망의 기본 구조와 학습 알고리즘

domain:
  id: deep_learning
  name: "딥러닝 기초"
  description: "신경망 구조, 학습 알고리즘, CNN, RNN 등 핵심 개념"

# =============================================================================
# 노드 정의
# =============================================================================
nodes:
  # ---------------------------------------------------------------------------
  # 신경망 기초 (Neural Network Basics)
  # ---------------------------------------------------------------------------
  - id: perceptron
    name: "퍼셉트론"
    category: nn_basics
    depth: 0
    mastery: 0
    last_reviewed: null
    
    prerequisites: []
    external_prerequisites:
      - domain: math_foundations
        node: linear_transformation
    
    leads_to:
      - activation_function
      - neural_network_layer
    
    core_concepts:
      - "가중합 + bias"
      - "단일 뉴런의 구조"
      - "선형 분류기로서의 퍼셉트론"
    
    quiz:
      - question: "퍼셉트론이 XOR 문제를 풀 수 없는 이유는?"
        answer: "XOR은 선형 분리 불가능하고, 퍼셉트론은 선형 결정 경계만 형성"
        type: conceptual
      - question: "퍼셉트론의 출력 계산식은?"
        answer: "y = activation(Σ(w_i × x_i) + b)"
        type: calculation
      - question: "왜 여러 층이 필요한가요?"
        answer: "비선형 문제를 풀기 위해 (층 + 비선형 활성화의 조합)"
        type: application

  - id: activation_function
    name: "활성화 함수"
    category: nn_basics
    depth: 1
    mastery: 0
    last_reviewed: null
    
    prerequisites:
      - perceptron
    
    leads_to:
      - relu
      - sigmoid
      - tanh
      - softmax
    
    core_concepts:
      - "비선형성 도입의 이유"
      - "미분 가능성의 중요성"
      - "출력 범위 조절"
    
    quiz:
      - question: "활성화 함수 없이 여러 층을 쌓으면 어떻게 되나요?"
        answer: "결국 하나의 선형 변환과 동일 (표현력 증가 없음)"
        type: conceptual
      - question: "활성화 함수가 미분 가능해야 하는 이유는?"
        answer: "역전파로 gradient 계산 시 미분이 필요"
        type: application

  - id: relu
    name: "ReLU"
    category: nn_basics
    depth: 2
    mastery: 0
    last_reviewed: null
    
    prerequisites:
      - activation_function
    
    leads_to:
      - vanishing_gradient
      - leaky_relu
    
    core_concepts:
      - "max(0, x)"
      - "계산 효율성"
      - "Dying ReLU 문제"
    
    quiz:
      - question: "ReLU(−5)의 값은?"
        answer: "0"
        type: calculation
      - question: "ReLU가 sigmoid보다 학습이 빠른 이유는?"
        answer: "x>0에서 gradient가 1로 일정하여 vanishing gradient 완화"
        type: conceptual
      - question: "Dying ReLU 문제란?"
        answer: "음수 입력에서 gradient=0이라 뉴런이 영구히 비활성화될 수 있음"
        type: application

  - id: sigmoid
    name: "Sigmoid"
    category: nn_basics
    depth: 2
    mastery: 0
    last_reviewed: null
    
    prerequisites:
      - activation_function
    external_prerequisites:
      - domain: math_foundations
        node: probability_basics
    
    leads_to:
      - binary_classification
      - vanishing_gradient
    
    core_concepts:
      - "σ(x) = 1/(1+e^(-x))"
      - "출력 범위 (0, 1)"
      - "확률 해석"
    
    quiz:
      - question: "sigmoid(0)의 값은?"
        answer: "0.5"
        type: calculation
      - question: "sigmoid의 gradient가 최대인 지점은?"
        answer: "x=0 (σ'(0) = 0.25)"
        type: conceptual
      - question: "이진 분류 출력층에 sigmoid를 쓰는 이유는?"
        answer: "출력을 0~1 사이 확률로 해석 가능"
        type: application

  - id: softmax
    name: "Softmax"
    category: nn_basics
    depth: 2
    mastery: 0
    last_reviewed: null
    
    prerequisites:
      - activation_function
    external_prerequisites:
      - domain: math_foundations
        node: probability_basics
    
    leads_to:
      - cross_entropy
      - temperature_sampling  # llm으로 연결
    
    core_concepts:
      - "exp(x_i) / Σexp(x_j)"
      - "다중 클래스 확률 분포"
      - "temperature 스케일링"
    
    quiz:
      - question: "softmax 출력의 합은 항상?"
        answer: "1"
        type: calculation
      - question: "temperature를 높이면 분포가 어떻게 변하나요?"
        answer: "더 균등해짐 (flat), 낮추면 더 뾰족해짐 (confident)"
        type: conceptual
      - question: "LLM에서 temperature 조절의 효과는?"
        answer: "높으면 다양한 출력, 낮으면 결정적(deterministic) 출력"
        type: application

  - id: neural_network_layer
    name: "신경망 층 (Layer)"
    category: nn_basics
    depth: 1
    mastery: 0
    last_reviewed: null
    
    prerequisites:
      - perceptron
    external_prerequisites:
      - domain: math_foundations
        node: linear_transformation
    
    leads_to:
      - forward_pass
      - mlp
    
    core_concepts:
      - "Linear: y = Wx + b"
      - "입력/출력 차원"
      - "파라미터 수 계산"
    
    quiz:
      - question: "(784→256) Linear layer의 파라미터 수는?"
        answer: "784×256 + 256 = 201,024"
        type: calculation
      - question: "hidden layer의 차원을 어떻게 정하나요?"
        answer: "경험적 선택, 입출력 사이 값, 실험으로 튜닝"
        type: application

  - id: mlp
    name: "다층 퍼셉트론 (MLP)"
    category: nn_basics
    depth: 2
    mastery: 0
    last_reviewed: null
    
    prerequisites:
      - neural_network_layer
      - activation_function
    
    leads_to:
      - forward_pass
      - backpropagation
      - feed_forward  # transformer의 FFN
    
    core_concepts:
      - "여러 층의 구성"
      - "Universal Approximation Theorem"
      - "깊이 vs 너비"
    
    quiz:
      - question: "Universal Approximation Theorem이란?"
        answer: "충분히 넓은 1개 hidden layer MLP로 어떤 연속함수도 근사 가능"
        type: conceptual
      - question: "왜 실제로는 깊은 네트워크를 쓰나요?"
        answer: "같은 표현력에 더 적은 파라미터, 계층적 특징 학습"
        type: application

  # ---------------------------------------------------------------------------
  # 학습 알고리즘 (Learning Algorithms)
  # ---------------------------------------------------------------------------
  - id: loss_function
    name: "손실 함수"
    category: learning
    depth: 0
    mastery: 0
    last_reviewed: null
    
    prerequisites: []
    
    leads_to:
      - cross_entropy
      - mse_loss
    
    core_concepts:
      - "예측과 정답의 차이 측정"
      - "미분 가능해야 함"
      - "문제 유형별 적합한 손실함수"
    
    quiz:
      - question: "손실함수가 미분 가능해야 하는 이유는?"
        answer: "gradient descent로 최적화하려면 gradient 계산 필요"
        type: conceptual
      - question: "손실이 0에 가까워지면 좋은 건가요?"
        answer: "학습 데이터에서는 그렇지만, 과적합 가능성도 있음"
        type: application

  - id: cross_entropy
    name: "Cross-Entropy Loss"
    category: learning
    depth: 1
    mastery: 0
    last_reviewed: null
    
    prerequisites:
      - loss_function
      - softmax
    external_prerequisites:
      - domain: math_foundations
        node: maximum_likelihood
    
    leads_to:
      - gradient_descent
      - language_model_loss  # llm으로 연결
    
    core_concepts:
      - "−Σ y_true × log(y_pred)"
      - "분류 문제의 표준 손실"
      - "MLE와의 관계"
    
    quiz:
      - question: "정답 확률이 1에 가까우면 cross-entropy는?"
        answer: "0에 가까움 (−log(1) = 0)"
        type: calculation
      - question: "cross-entropy 최소화가 MLE와 같은 이유는?"
        answer: "정답의 log-likelihood를 최대화하는 것과 수학적으로 동일"
        type: conceptual
      - question: "LLM의 next token prediction loss는 어떤 형태인가요?"
        answer: "Cross-entropy (정답 토큰의 확률을 최대화)"
        type: application

  - id: gradient_descent
    name: "경사하강법"
    category: learning
    depth: 1
    mastery: 0
    last_reviewed: null
    
    prerequisites:
      - loss_function
    external_prerequisites:
      - domain: math_foundations
        node: gradient
    
    leads_to:
      - learning_rate
      - sgd
      - backpropagation
    
    core_concepts:
      - "θ = θ − α∇L"
      - "gradient 반대 방향 이동"
      - "local minimum 수렴"
    
    quiz:
      - question: "gradient의 반대 방향으로 이동하는 이유는?"
        answer: "gradient는 증가 방향, 손실을 줄이려면 반대로"
        type: conceptual
      - question: "learning rate가 너무 크면?"
        answer: "발산하거나 최솟값을 지나침"
        type: application
      - question: "learning rate가 너무 작으면?"
        answer: "수렴이 매우 느리고, local minimum에 갇힐 수 있음"
        type: application

  - id: learning_rate
    name: "학습률 (Learning Rate)"
    category: learning
    depth: 2
    mastery: 0
    last_reviewed: null
    
    prerequisites:
      - gradient_descent
    
    leads_to:
      - lr_scheduler
      - adam
    
    core_concepts:
      - "step size 조절"
      - "적절한 값 찾기의 어려움"
      - "warmup, decay 전략"
    
    quiz:
      - question: "일반적인 초기 learning rate 범위는?"
        answer: "0.001 ~ 0.01 (Adam 기준), 0.01 ~ 0.1 (SGD)"
        type: knowledge
      - question: "learning rate warmup의 목적은?"
        answer: "초기 불안정한 gradient에서 급격한 업데이트 방지"
        type: application

  - id: sgd
    name: "Stochastic Gradient Descent"
    category: learning
    depth: 2
    mastery: 0
    last_reviewed: null
    
    prerequisites:
      - gradient_descent
    
    leads_to:
      - momentum
      - mini_batch
    
    core_concepts:
      - "전체 대신 샘플/배치로 gradient 추정"
      - "noise로 인한 local minimum 탈출"
      - "계산 효율성"
    
    quiz:
      - question: "SGD가 full batch GD보다 나은 점은?"
        answer: "빠른 업데이트, 메모리 효율, noise로 local min 탈출 가능"
        type: conceptual
      - question: "SGD의 단점은?"
        answer: "gradient 추정의 분산이 커서 수렴이 불안정할 수 있음"
        type: application

  - id: momentum
    name: "Momentum"
    category: learning
    depth: 3
    mastery: 0
    last_reviewed: null
    
    prerequisites:
      - sgd
    
    leads_to:
      - adam
    
    core_concepts:
      - "v = βv + ∇L, θ = θ − αv"
      - "관성을 이용한 가속"
      - "진동 감소"
    
    quiz:
      - question: "momentum의 물리적 비유는?"
        answer: "공이 언덕을 굴러 내려갈 때 관성으로 가속"
        type: conceptual
      - question: "momentum이 수렴을 빠르게 하는 원리는?"
        answer: "일관된 방향은 가속, 진동하는 방향은 상쇄"
        type: application

  - id: adam
    name: "Adam Optimizer"
    category: learning
    depth: 3
    mastery: 0
    last_reviewed: null
    
    prerequisites:
      - momentum
      - learning_rate
    
    leads_to:
      - adamw
    
    core_concepts:
      - "momentum + RMSprop"
      - "적응적 학습률"
      - "m, v의 bias correction"
    
    quiz:
      - question: "Adam의 두 가지 moving average는?"
        answer: "1차 모멘트(gradient 평균), 2차 모멘트(gradient 제곱 평균)"
        type: conceptual
      - question: "Adam이 SGD+momentum보다 좋은 경우는?"
        answer: "sparse gradient, 다양한 스케일의 파라미터, 튜닝 시간 부족 시"
        type: application
      - question: "Adam의 기본 하이퍼파라미터는?"
        answer: "β1=0.9, β2=0.999, ε=1e-8"
        type: knowledge

  - id: adamw
    name: "AdamW"
    category: learning
    depth: 4
    mastery: 0
    last_reviewed: null
    
    prerequisites:
      - adam
      - regularization
    
    leads_to:
      - llm_training  # llm으로 연결
    
    core_concepts:
      - "decoupled weight decay"
      - "Adam + proper L2 regularization"
      - "LLM 학습의 표준"
    
    quiz:
      - question: "Adam에서 weight decay를 적용하면 문제점은?"
        answer: "적응적 학습률과 결합되어 의도한 정규화 효과가 안 남"
        type: conceptual
      - question: "AdamW가 LLM 학습의 표준인 이유는?"
        answer: "대규모 모델에서 안정적인 수렴과 일반화 성능"
        type: application

  - id: backpropagation
    name: "역전파 (Backpropagation)"
    category: learning
    depth: 2
    mastery: 0
    last_reviewed: null
    
    prerequisites:
      - gradient_descent
      - mlp
    external_prerequisites:
      - domain: math_foundations
        node: chain_rule
    
    leads_to:
      - vanishing_gradient
      - computational_graph
    
    core_concepts:
      - "chain rule의 반복 적용"
      - "출력→입력 방향 gradient 전파"
      - "forward → backward pass"
    
    quiz:
      - question: "역전파에서 chain rule이 핵심인 이유는?"
        answer: "각 층의 gradient를 연쇄적으로 곱해 입력까지 전파"
        type: conceptual
      - question: "forward pass와 backward pass의 순서는?"
        answer: "forward로 출력 계산 → backward로 gradient 계산"
        type: knowledge
      - question: "역전파의 시간 복잡도가 forward와 비슷한 이유는?"
        answer: "각 연산의 gradient를 한 번씩만 계산 (dynamic programming)"
        type: application

  - id: computational_graph
    name: "계산 그래프"
    category: learning
    depth: 3
    mastery: 0
    last_reviewed: null
    
    prerequisites:
      - backpropagation
    
    leads_to:
      - autograd
    
    core_concepts:
      - "연산을 노드로, 데이터 흐름을 엣지로"
      - "자동 미분의 기반"
      - "PyTorch의 dynamic graph"
    
    quiz:
      - question: "계산 그래프의 장점은?"
        answer: "복잡한 연산의 gradient를 자동으로 추적/계산 가능"
        type: conceptual
      - question: "PyTorch가 dynamic graph를 쓰는 장점은?"
        answer: "control flow (if, for)를 자연스럽게 포함, 디버깅 용이"
        type: application

  # ---------------------------------------------------------------------------
  # 정규화 기법 (Regularization)
  # ---------------------------------------------------------------------------
  - id: overfitting
    name: "과적합 (Overfitting)"
    category: regularization
    depth: 0
    mastery: 0
    last_reviewed: null
    
    prerequisites: []
    
    leads_to:
      - regularization
      - dropout
      - early_stopping
    
    core_concepts:
      - "학습 데이터에 과하게 맞춤"
      - "일반화 성능 저하"
      - "train/val loss 괴리"
    
    quiz:
      - question: "train loss는 낮은데 val loss가 높으면?"
        answer: "과적합 (모델이 학습 데이터를 암기)"
        type: conceptual
      - question: "과적합을 줄이는 방법들은?"
        answer: "정규화, dropout, 데이터 증강, early stopping, 모델 축소"
        type: application

  - id: regularization
    name: "정규화 (Regularization)"
    category: regularization
    depth: 1
    mastery: 0
    last_reviewed: null
    
    prerequisites:
      - overfitting
    external_prerequisites:
      - domain: math_foundations
        node: vector_norm
    
    leads_to:
      - dropout
      - weight_decay
    
    core_concepts:
      - "L1/L2 penalty"
      - "모델 복잡도 제한"
      - "loss + λ×penalty"
    
    quiz:
      - question: "L1 정규화의 특징은?"
        answer: "sparse solution 유도 (일부 가중치를 0으로)"
        type: conceptual
      - question: "L2 정규화의 효과는?"
        answer: "가중치를 작게 유지하여 과적합 방지"
        type: application

  - id: dropout
    name: "Dropout"
    category: regularization
    depth: 2
    mastery: 0
    last_reviewed: null
    
    prerequisites:
      - regularization
      - mlp
    
    leads_to: []
    
    core_concepts:
      - "학습 시 무작위로 뉴런 비활성화"
      - "앙상블 효과"
      - "inference 시 scaling"
    
    quiz:
      - question: "dropout rate 0.5의 의미는?"
        answer: "각 뉴런이 50% 확률로 비활성화"
        type: knowledge
      - question: "inference 시 dropout을 끄고 출력을 조정하는 이유는?"
        answer: "학습 시보다 많은 뉴런이 활성화되므로 스케일 맞춤"
        type: conceptual
      - question: "dropout이 앙상블과 비슷한 이유는?"
        answer: "매 학습마다 다른 서브네트워크를 학습하는 효과"
        type: application

  - id: batch_normalization
    name: "Batch Normalization"
    category: regularization
    depth: 2
    mastery: 0
    last_reviewed: null
    
    prerequisites:
      - mlp
    external_prerequisites:
      - domain: math_foundations
        node: variance
    
    leads_to:
      - layer_normalization
    
    core_concepts:
      - "배치 내 정규화 (평균=0, 분산=1)"
      - "학습 안정화"
      - "γ, β learnable parameters"
    
    quiz:
      - question: "BatchNorm이 학습을 안정화하는 원리는?"
        answer: "internal covariate shift 감소, gradient flow 개선"
        type: conceptual
      - question: "BatchNorm의 γ, β 파라미터의 역할은?"
        answer: "정규화 후 scale과 shift를 학습하여 표현력 유지"
        type: application
      - question: "BatchNorm을 inference에서 쓸 때 주의점은?"
        answer: "학습 중 계산된 running mean/var 사용 (배치 통계 아님)"
        type: knowledge

  - id: layer_normalization
    name: "Layer Normalization"
    category: regularization
    depth: 3
    mastery: 0
    last_reviewed: null
    
    prerequisites:
      - batch_normalization
    
    leads_to:
      - transformer_architecture  # transformer로 연결
    
    core_concepts:
      - "각 샘플 내에서 정규화"
      - "배치 크기 무관"
      - "Transformer의 표준"
    
    quiz:
      - question: "LayerNorm이 Transformer에서 BatchNorm 대신 쓰이는 이유는?"
        answer: "sequence 길이가 가변적이고, 배치 통계에 의존하면 안 됨"
        type: conceptual
      - question: "LayerNorm의 정규화 축은?"
        answer: "feature dimension (배치와 sequence 축은 독립)"
        type: knowledge

  # ---------------------------------------------------------------------------
  # 주요 문제와 해결책
  # ---------------------------------------------------------------------------
  - id: vanishing_gradient
    name: "기울기 소실 문제"
    category: problems
    depth: 3
    mastery: 0
    last_reviewed: null
    
    prerequisites:
      - backpropagation
      - sigmoid
    
    leads_to:
      - residual_connection
      - lstm
    
    core_concepts:
      - "깊은 층에서 gradient가 0에 수렴"
      - "sigmoid/tanh의 saturation"
      - "학습 정체"
    
    quiz:
      - question: "vanishing gradient의 원인은?"
        answer: "chain rule로 작은 gradient들이 계속 곱해짐"
        type: conceptual
      - question: "ReLU가 이 문제를 완화하는 이유는?"
        answer: "양수 영역에서 gradient가 1로 일정"
        type: application
      - question: "100층 네트워크에서 sigmoid를 쓰면 어떻게 되나요?"
        answer: "입력층 근처의 gradient가 거의 0이 되어 학습 불가"
        type: application

  - id: residual_connection
    name: "잔차 연결 (Residual Connection)"
    category: problems
    depth: 4
    mastery: 0
    last_reviewed: null
    
    prerequisites:
      - vanishing_gradient
    
    leads_to:
      - resnet
      - transformer_architecture  # transformer로 연결
    
    core_concepts:
      - "y = F(x) + x"
      - "gradient highway"
      - "identity mapping"
    
    quiz:
      - question: "residual connection이 gradient flow를 돕는 원리는?"
        answer: "skip connection으로 gradient가 직접 전파되는 경로 제공"
        type: conceptual
      - question: "ResNet이 1000층도 학습 가능한 이유는?"
        answer: "residual connection이 vanishing gradient 방지"
        type: application

  # ---------------------------------------------------------------------------
  # CNN
  # ---------------------------------------------------------------------------
  - id: convolution
    name: "합성곱 (Convolution)"
    category: cnn
    depth: 2
    mastery: 0
    last_reviewed: null
    
    prerequisites:
      - neural_network_layer
    
    leads_to:
      - cnn_architecture
      - pooling
    
    core_concepts:
      - "커널과 입력의 element-wise 곱의 합"
      - "weight sharing"
      - "지역적 패턴 감지"
    
    quiz:
      - question: "3x3 커널로 5x5 입력을 convolve하면 출력 크기는? (stride=1, no padding)"
        answer: "3x3"
        type: calculation
      - question: "convolution의 weight sharing 장점은?"
        answer: "파라미터 수 대폭 감소, 위치 불변성"
        type: conceptual
      - question: "이미지 처리에 FC 대신 Conv를 쓰는 이유는?"
        answer: "공간적 구조 보존, 파라미터 효율, 지역 패턴 학습"
        type: application

  - id: pooling
    name: "Pooling"
    category: cnn
    depth: 3
    mastery: 0
    last_reviewed: null
    
    prerequisites:
      - convolution
    
    leads_to:
      - cnn_architecture
    
    core_concepts:
      - "다운샘플링"
      - "max pooling vs average pooling"
      - "위치 불변성 증가"
    
    quiz:
      - question: "2x2 max pooling의 효과는?"
        answer: "각 2x2 영역에서 최댓값만 남겨 크기 1/4로 축소"
        type: calculation
      - question: "pooling이 translation invariance에 기여하는 이유는?"
        answer: "약간의 위치 변화에도 같은 최댓값 선택 가능"
        type: conceptual

  - id: cnn_architecture
    name: "CNN 아키텍처"
    category: cnn
    depth: 4
    mastery: 0
    last_reviewed: null
    
    prerequisites:
      - convolution
      - pooling
    
    leads_to:
      - resnet
      - vision_transformer  # transformer로 연결 (optional)
    
    core_concepts:
      - "Conv → Pool → Conv → Pool → FC"
      - "계층적 특징 학습"
      - "receptive field"
    
    quiz:
      - question: "CNN이 이미지에서 계층적 특징을 학습한다는 의미는?"
        answer: "얕은 층: edge, 깊은 층: 복잡한 패턴 (눈, 얼굴 등)"
        type: conceptual
      - question: "receptive field란?"
        answer: "한 출력 뉴런이 보는 입력 영역의 크기"
        type: knowledge

  - id: resnet
    name: "ResNet"
    category: cnn
    depth: 5
    mastery: 0
    last_reviewed: null
    
    prerequisites:
      - cnn_architecture
      - residual_connection
    
    leads_to: []
    
    core_concepts:
      - "Deep residual learning"
      - "skip connection 블록"
      - "152층 이상 가능"
    
    quiz:
      - question: "ResNet이 VGG보다 깊으면서도 학습이 쉬운 이유는?"
        answer: "residual connection이 gradient degradation 방지"
        type: conceptual
      - question: "ResNet의 기본 블록 구조는?"
        answer: "Conv → BN → ReLU → Conv → BN → (+x) → ReLU"
        type: knowledge

  # ---------------------------------------------------------------------------
  # RNN
  # ---------------------------------------------------------------------------
  - id: rnn
    name: "순환 신경망 (RNN)"
    category: rnn
    depth: 2
    mastery: 0
    last_reviewed: null
    
    prerequisites:
      - mlp
    
    leads_to:
      - lstm
      - gru
      - seq2seq
    
    core_concepts:
      - "h_t = f(h_{t-1}, x_t)"
      - "시퀀스 처리"
      - "가변 길이 입력"
    
    quiz:
      - question: "RNN이 MLP와 다른 핵심 차이는?"
        answer: "hidden state가 시간에 따라 전달되어 시퀀스 정보 유지"
        type: conceptual
      - question: "vanilla RNN의 한계는?"
        answer: "vanishing gradient로 긴 의존성 학습 어려움"
        type: application

  - id: lstm
    name: "LSTM"
    category: rnn
    depth: 3
    mastery: 0
    last_reviewed: null
    
    prerequisites:
      - rnn
      - vanishing_gradient
    
    leads_to:
      - seq2seq
    
    core_concepts:
      - "cell state + hidden state"
      - "forget/input/output gate"
      - "장기 의존성 학습"
    
    quiz:
      - question: "LSTM의 forget gate 역할은?"
        answer: "이전 cell state에서 버릴 정보를 결정"
        type: conceptual
      - question: "LSTM이 긴 시퀀스에서 RNN보다 나은 이유는?"
        answer: "cell state가 gradient highway 역할, gate로 정보 흐름 제어"
        type: application
      - question: "LSTM의 gate 수는?"
        answer: "3개 (forget, input, output)"
        type: knowledge

  - id: seq2seq
    name: "Sequence-to-Sequence"
    category: rnn
    depth: 4
    mastery: 0
    last_reviewed: null
    
    prerequisites:
      - lstm
    
    leads_to:
      - attention_mechanism  # transformer로 연결
    
    core_concepts:
      - "Encoder-Decoder 구조"
      - "context vector"
      - "가변 길이 입출력"
    
    quiz:
      - question: "seq2seq의 bottleneck 문제란?"
        answer: "전체 입력을 하나의 고정 크기 context vector로 압축해야 함"
        type: conceptual
      - question: "이 bottleneck을 해결한 방법은?"
        answer: "Attention mechanism"
        type: application

# =============================================================================
# 카테고리별 노드 그룹
# =============================================================================
categories:
  nn_basics:
    name: "신경망 기초"
    color: "#FF6B6B"
    nodes: [perceptron, activation_function, relu, sigmoid, softmax, 
            neural_network_layer, mlp]
    
  learning:
    name: "학습 알고리즘"
    color: "#4ECDC4"
    nodes: [loss_function, cross_entropy, gradient_descent, learning_rate,
            sgd, momentum, adam, adamw, backpropagation, computational_graph]
    
  regularization:
    name: "정규화"
    color: "#45B7D1"
    nodes: [overfitting, regularization, dropout, batch_normalization, 
            layer_normalization]
    
  problems:
    name: "문제와 해결책"
    color: "#96CEB4"
    nodes: [vanishing_gradient, residual_connection]
    
  cnn:
    name: "CNN"
    color: "#FFEAA7"
    nodes: [convolution, pooling, cnn_architecture, resnet]
    
  rnn:
    name: "RNN"
    color: "#DDA0DD"
    nodes: [rnn, lstm, seq2seq]

# =============================================================================
# 도메인 간 연결점
# =============================================================================
cross_domain_connections:
  from_math_foundations:
    - from: gradient
      to: gradient_descent
    - from: chain_rule
      to: backpropagation
    - from: linear_transformation
      to: neural_network_layer
    - from: variance
      to: batch_normalization
    - from: maximum_likelihood
      to: cross_entropy
    - from: vector_norm
      to: regularization
    - from: probability_basics
      to: sigmoid
    - from: probability_basics
      to: softmax
      
  to_transformer:
    - from: mlp
      to: feed_forward
      notes: "MLP → Transformer FFN"
    - from: layer_normalization
      to: transformer_architecture
      notes: "LayerNorm → Transformer의 정규화"
    - from: residual_connection
      to: transformer_architecture
      notes: "Residual → Transformer의 skip connection"
    - from: softmax
      to: attention_mechanism
      notes: "Softmax → Attention weights"
    - from: seq2seq
      to: attention_mechanism
      notes: "Seq2seq의 한계 → Attention 도입"
      
  to_llm:
    - from: adamw
      to: llm_training
      notes: "AdamW → LLM 학습의 표준 optimizer"
    - from: cross_entropy
      to: language_model_loss
      notes: "CE loss → LLM의 next token prediction"
    - from: softmax
      to: temperature_sampling
      notes: "Softmax → 샘플링 temperature"

# Hugging Face LLM Course Chapter 3.4 - Full Training Loop ìš”ì•½

> **ëª©í‘œ**: Trainer API ì—†ì´ ìˆœìˆ˜ PyTorchë¡œ fine-tuning ì „ì²´ ê³¼ì •ì„ ì§ì ‘ êµ¬í˜„í•˜ê¸°

---

[A full traiding loop](https://huggingface.co/learn/llm-course/chapter3/4) ìš”ì•½

## ì „ì²´ íŒŒì´í”„ë¼ì¸ ê°œìš”

```
ë°ì´í„° ì¤€ë¹„ â†’ ëª¨ë¸ ë¡œë“œ â†’ í•™ìŠµ ì„¤ì • â†’ í•™ìŠµ ë£¨í”„ â†’ í‰ê°€ ë£¨í”„
```

---

## 1ë‹¨ê³„: ë°ì´í„° ì¤€ë¹„ (Data Preparation)

### 1.1 ë°ì´í„°ì…‹ ë¡œë“œ ë° í† í°í™”

```python
from datasets import load_dataset
from transformers import AutoTokenizer, DataCollatorWithPadding

raw_datasets = load_dataset("glue", "mrpc")
checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)

def tokenize_function(example):
    return tokenizer(example["sentence1"], example["sentence2"], truncation=True)

tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
```

**ê° ì½”ë“œì˜ ì—­í• :**

| ì½”ë“œ | ì—­í•  | ì™œ í•„ìš”í•œê°€? |
|------|------|-------------|
| `load_dataset("glue", "mrpc")` | GLUE ë²¤ì¹˜ë§ˆí¬ì˜ MRPC(ë¬¸ì¥ ìœ ì‚¬ë„) ë°ì´í„°ì…‹ ë¡œë“œ | í•™ìŠµ/ê²€ì¦ ë°ì´í„° í™•ë³´ |
| `AutoTokenizer.from_pretrained(checkpoint)` | BERT ëª¨ë¸ì— ë§ëŠ” í† í¬ë‚˜ì´ì € ë¡œë“œ | ëª¨ë¸ì´ ì´í•´í•  ìˆ˜ ìˆëŠ” í˜•íƒœë¡œ í…ìŠ¤íŠ¸ ë³€í™˜ |
| `tokenize_function` | ë‘ ë¬¸ì¥ì„ í•¨ê»˜ í† í°í™” | BERTëŠ” [SEP] í† í°ìœ¼ë¡œ êµ¬ë¶„ëœ ë¬¸ì¥ ìŒì„ ì…ë ¥ë°›ìŒ |
| `raw_datasets.map(..., batched=True)` | ì „ì²´ ë°ì´í„°ì…‹ì— í† í°í™” í•¨ìˆ˜ ì ìš© | ë°°ì¹˜ ì²˜ë¦¬ë¡œ ì†ë„ í–¥ìƒ |
| `DataCollatorWithPadding` | ë°°ì¹˜ ë‚´ ì‹œí€€ìŠ¤ ê¸¸ì´ë¥¼ ë™ì ìœ¼ë¡œ íŒ¨ë”© | GPU íš¨ìœ¨ì  ì²˜ë¦¬ë¥¼ ìœ„í•´ ë™ì¼ ê¸¸ì´ í•„ìš” |

### 1.2 ë°ì´í„°ì…‹ í›„ì²˜ë¦¬ (ëª¨ë¸ ì…ë ¥ í˜•ì‹ ë§ì¶”ê¸°)

```python
tokenized_datasets = tokenized_datasets.remove_columns(["sentence1", "sentence2", "idx"])
tokenized_datasets = tokenized_datasets.rename_column("label", "labels")
tokenized_datasets.set_format("torch")
```

**ê° ì½”ë“œì˜ ì—­í• :**

| ì½”ë“œ | ì—­í•  | ì™œ í•„ìš”í•œê°€? |
|------|------|-------------|
| `remove_columns([...])` | ì›ë³¸ í…ìŠ¤íŠ¸ ì»¬ëŸ¼ ì œê±° | ëª¨ë¸ì€ í† í°í™”ëœ ìˆ«ìë§Œ í•„ìš”, ë¶ˆí•„ìš”í•œ ë°ì´í„° ì œê±° |
| `rename_column("label", "labels")` | ì»¬ëŸ¼ëª… ë³€ê²½ | HuggingFace ëª¨ë¸ì€ `labels`ë¼ëŠ” ì´ë¦„ì„ ê¸°ëŒ€í•¨ |
| `set_format("torch")` | ë°˜í™˜ í˜•ì‹ì„ PyTorch í…ì„œë¡œ ì„¤ì • | PyTorch í•™ìŠµ ë£¨í”„ì—ì„œ ë°”ë¡œ ì‚¬ìš© ê°€ëŠ¥ |

**ì²˜ë¦¬ ì „í›„ ë¹„êµ:**
```
ì²˜ë¦¬ ì „: ["sentence1", "sentence2", "idx", "label", "input_ids", "attention_mask", "token_type_ids"]
ì²˜ë¦¬ í›„: ["labels", "input_ids", "attention_mask", "token_type_ids"]
```

### 1.3 DataLoader ìƒì„±

```python
from torch.utils.data import DataLoader

train_dataloader = DataLoader(
    tokenized_datasets["train"], shuffle=True, batch_size=8, collate_fn=data_collator
)
eval_dataloader = DataLoader(
    tokenized_datasets["validation"], batch_size=8, collate_fn=data_collator
)
```

**ê° íŒŒë¼ë¯¸í„°ì˜ ì—­í• :**

| íŒŒë¼ë¯¸í„° | ê°’ | ì—­í•  |
|----------|-----|------|
| `shuffle=True` | í•™ìŠµ ë°ì´í„°ë§Œ | ë§¤ ì—í­ë§ˆë‹¤ ë°ì´í„° ìˆœì„œë¥¼ ì„ì–´ ê³¼ì í•© ë°©ì§€ |
| `batch_size=8` | 8ê°œì”© ë¬¶ìŒ | GPU ë©”ëª¨ë¦¬ì™€ í•™ìŠµ ì•ˆì •ì„±ì˜ ê· í˜• |
| `collate_fn=data_collator` | ë™ì  íŒ¨ë”© í•¨ìˆ˜ | ë°°ì¹˜ ë‚´ ìµœëŒ€ ê¸¸ì´ì— ë§ì¶° íŒ¨ë”© |

### 1.4 ë°°ì¹˜ êµ¬ì¡° í™•ì¸

```python
for batch in train_dataloader:
    break
{k: v.shape for k, v in batch.items()}
```

**ì¶œë ¥ ì˜ˆì‹œ:**
```python
{
    'attention_mask': torch.Size([8, 65]),   # 8ê°œ ìƒ˜í”Œ, ìµœëŒ€ 65 í† í°
    'input_ids': torch.Size([8, 65]),        # í† í° ID
    'labels': torch.Size([8]),               # ê° ìƒ˜í”Œì˜ ì •ë‹µ ë ˆì´ë¸”
    'token_type_ids': torch.Size([8, 65])    # ë¬¸ì¥ êµ¬ë¶„ (0: ì²«ë²ˆì§¸, 1: ë‘ë²ˆì§¸)
}
```

---

## 2ë‹¨ê³„: ëª¨ë¸ ë° í•™ìŠµ ì„¤ì •

### 2.1 ëª¨ë¸ ë¡œë“œ

```python
from transformers import AutoModelForSequenceClassification

model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
```

**ì—­í• :**
- ì‚¬ì „í•™ìŠµëœ BERT ìœ„ì— ë¶„ë¥˜ í—¤ë“œ(classification head) ì¶”ê°€
- `num_labels=2`: MRPCëŠ” ì´ì§„ ë¶„ë¥˜ (ìœ ì‚¬/ë¹„ìœ ì‚¬)

**ëª¨ë¸ êµ¬ì¡°:**
```
[BERT Encoder] â†’ [Pooler] â†’ [Classification Head (768â†’2)]
     â†“                              â†“
 Hidden States              Logits (2ê°œ í´ë˜ìŠ¤)
```

### 2.2 ëª¨ë¸ ë™ì‘ í™•ì¸

```python
outputs = model(**batch)
print(outputs.loss, outputs.logits.shape)
# tensor(0.5441, grad_fn=<NllLossBackward>) torch.Size([8, 2])
```

**í•µì‹¬ í¬ì¸íŠ¸:**
- `labels`ê°€ ì œê³µë˜ë©´ ìë™ìœ¼ë¡œ loss ê³„ì‚°
- `logits`: ê° í´ë˜ìŠ¤ì— ëŒ€í•œ ì ìˆ˜ (softmax ì ìš© ì „)
- `grad_fn`: ì—­ì „íŒŒë¥¼ ìœ„í•œ ì—°ì‚° ê·¸ë˜í”„ ì—°ê²°ë¨

### 2.3 Optimizer ì„¤ì •

```python
from torch.optim import AdamW

optimizer = AdamW(model.parameters(), lr=5e-5)
```

**AdamW vs Adam:**
- AdamW = Adam + Decoupled Weight Decay
- Weight Decay: í° ê°€ì¤‘ì¹˜ì— íŒ¨ë„í‹°ë¥¼ ì¤˜ì„œ ê³¼ì í•© ë°©ì§€
- Decoupled: weight decayë¥¼ gradientê°€ ì•„ë‹Œ ê°€ì¤‘ì¹˜ì— ì§ì ‘ ì ìš©

### 2.4 Learning Rate Scheduler ì„¤ì •

```python
from transformers import get_scheduler

num_epochs = 3
num_training_steps = num_epochs * len(train_dataloader)

lr_scheduler = get_scheduler(
    "linear",
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=num_training_steps,
)
print(num_training_steps)  # 1377
```

**Linear Scheduler ë™ì‘:**
```
Learning Rate
    â”‚
5e-5â”œâ”€â”€â”€â”€â•²
    â”‚      â•²
    â”‚        â•²
    â”‚          â•²
  0 â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•²â”€â”€â”€â”€
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Steps
    0            1377
```

**ê³„ì‚° ì˜ˆì‹œ:**
- í•™ìŠµ ë°ì´í„°: ~3,668ê°œ
- batch_size: 8
- ë°°ì¹˜ ìˆ˜: 3,668 / 8 â‰ˆ 459
- ì´ ìŠ¤í…: 459 Ã— 3 epochs = 1,377

### 2.5 Device ì„¤ì •

```python
import torch

device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
model.to(device)
```

**ì—­í• :**
- GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
- ëª¨ë¸ì„ í•´ë‹¹ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
- CPU vs GPU: í•™ìŠµ ì‹œê°„ ìˆ˜ ì‹œê°„ vs ìˆ˜ ë¶„

---

## 3ë‹¨ê³„: í•™ìŠµ ë£¨í”„ (Training Loop)

```python
from tqdm.auto import tqdm

progress_bar = tqdm(range(num_training_steps))

model.train()  # â‘  í•™ìŠµ ëª¨ë“œ ì„¤ì •
for epoch in range(num_epochs):
    for batch in train_dataloader:
        batch = {k: v.to(device) for k, v in batch.items()}  # â‘¡ ë°ì´í„°ë¥¼ GPUë¡œ
        outputs = model(**batch)                              # â‘¢ Forward Pass
        loss = outputs.loss                                   # â‘£ Loss ì¶”ì¶œ
        loss.backward()                                       # â‘¤ Backward Pass

        optimizer.step()                                      # â‘¥ ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸
        lr_scheduler.step()                                   # â‘¦ LR ì¡°ì •
        optimizer.zero_grad()                                 # â‘§ Gradient ì´ˆê¸°í™”
        progress_bar.update(1)
```

### ê° ë‹¨ê³„ ìƒì„¸ ì„¤ëª…

| ë‹¨ê³„ | ì½”ë“œ | ì—­í•  | ìƒì„¸ ì„¤ëª… |
|------|------|------|----------|
| â‘  | `model.train()` | í•™ìŠµ ëª¨ë“œ í™œì„±í™” | Dropout, BatchNorm ë“±ì´ í•™ìŠµìš©ìœ¼ë¡œ ë™ì‘ |
| â‘¡ | `v.to(device)` | ë°ì´í„° GPU ì´ë™ | ëª¨ë¸ê³¼ ë°ì´í„°ê°€ ê°™ì€ ë””ë°”ì´ìŠ¤ì— ìˆì–´ì•¼ ì—°ì‚° ê°€ëŠ¥ |
| â‘¢ | `model(**batch)` | Forward Pass | ì…ë ¥ â†’ ì˜ˆì¸¡ê°’(logits) ê³„ì‚° |
| â‘£ | `outputs.loss` | Loss ì¶”ì¶œ | Cross-Entropy Loss (labelsê°€ ìˆìœ¼ë©´ ìë™ ê³„ì‚°) |
| â‘¤ | `loss.backward()` | Backward Pass | ê° íŒŒë¼ë¯¸í„°ì— ëŒ€í•œ gradient ê³„ì‚° |
| â‘¥ | `optimizer.step()` | ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸ | gradient ë°©í–¥ìœ¼ë¡œ íŒŒë¼ë¯¸í„° ì¡°ì • |
| â‘¦ | `lr_scheduler.step()` | Learning Rate ì¡°ì • | ìŠ¤ì¼€ì¤„ì— ë”°ë¼ LR ê°ì†Œ |
| â‘§ | `optimizer.zero_grad()` | Gradient ì´ˆê¸°í™” | ë‹¤ìŒ ë°°ì¹˜ë¥¼ ìœ„í•´ gradient ë¦¬ì…‹ |

### í•™ìŠµ ë£¨í”„ ìˆœì„œ (ì¤‘ìš”!)

```
Forward â†’ Backward â†’ Optimizer Step â†’ Scheduler Step â†’ Zero Grad
   â†“          â†“           â†“              â†“              â†“
ì˜ˆì¸¡ ê³„ì‚°   ê¸°ìš¸ê¸° ê³„ì‚°  ê°€ì¤‘ì¹˜ ìˆ˜ì •   LR ì¡°ì •      ê¸°ìš¸ê¸° ë¦¬ì…‹
```

âš ï¸ **ìˆœì„œê°€ ì¤‘ìš”í•œ ì´ìœ :**
- `zero_grad()`ë¥¼ ë¨¼ì € í•˜ë©´? â†’ ê³„ì‚°ëœ gradientê°€ ì‚¬ë¼ì§
- `optimizer.step()` ì „ì— `zero_grad()`í•˜ë©´? â†’ ì—…ë°ì´íŠ¸í•  gradientê°€ ì—†ìŒ

---

## 4ë‹¨ê³„: í‰ê°€ ë£¨í”„ (Evaluation Loop)

```python
import evaluate

metric = evaluate.load("glue", "mrpc")

model.eval()  # â‘  í‰ê°€ ëª¨ë“œ ì„¤ì •
for batch in eval_dataloader:
    batch = {k: v.to(device) for k, v in batch.items()}
    with torch.no_grad():  # â‘¡ Gradient ê³„ì‚° ë¹„í™œì„±í™”
        outputs = model(**batch)

    logits = outputs.logits
    predictions = torch.argmax(logits, dim=-1)  # â‘¢ ì˜ˆì¸¡ í´ë˜ìŠ¤ ì„ íƒ
    metric.add_batch(predictions=predictions, references=batch["labels"])  # â‘£ ê²°ê³¼ ëˆ„ì 

metric.compute()  # â‘¤ ìµœì¢… ë©”íŠ¸ë¦­ ê³„ì‚°
# {'accuracy': 0.8431372549019608, 'f1': 0.8907849829351535}
```

### ê° ë‹¨ê³„ ìƒì„¸ ì„¤ëª…

| ë‹¨ê³„ | ì½”ë“œ | ì—­í•  | ì™œ í•„ìš”í•œê°€? |
|------|------|------|-------------|
| â‘  | `model.eval()` | í‰ê°€ ëª¨ë“œ | Dropout ë¹„í™œì„±í™”, BatchNorm ê³ ì • |
| â‘¡ | `torch.no_grad()` | Gradient ê³„ì‚° OFF | ë©”ëª¨ë¦¬ ì ˆì•½ + ì†ë„ í–¥ìƒ (ì—­ì „íŒŒ ë¶ˆí•„ìš”) |
| â‘¢ | `torch.argmax(logits, dim=-1)` | ê°€ì¥ ë†’ì€ ì ìˆ˜ì˜ í´ë˜ìŠ¤ ì„ íƒ | logits â†’ ì‹¤ì œ ì˜ˆì¸¡ ë ˆì´ë¸” |
| â‘£ | `metric.add_batch()` | ë°°ì¹˜ë³„ ê²°ê³¼ ëˆ„ì  | ì „ì²´ ë°ì´í„°ì— ëŒ€í•œ ë©”íŠ¸ë¦­ ê³„ì‚° ì¤€ë¹„ |
| â‘¤ | `metric.compute()` | ìµœì¢… ë©”íŠ¸ë¦­ ê³„ì‚° | Accuracy, F1 Score ë°˜í™˜ |

### `model.train()` vs `model.eval()` ë¹„êµ

| ëª¨ë“œ | Dropout | BatchNorm | ìš©ë„ |
|------|---------|-----------|------|
| `train()` | í™œì„±í™” (ëœë¤ ë“œë¡­) | ë°°ì¹˜ í†µê³„ ì‚¬ìš© | í•™ìŠµ ì‹œ |
| `eval()` | ë¹„í™œì„±í™” (ëª¨ë“  ë‰´ëŸ° ì‚¬ìš©) | í•™ìŠµëœ í†µê³„ ì‚¬ìš© | í‰ê°€/ì¶”ë¡  ì‹œ |

---

## 5ë‹¨ê³„: ğŸ¤— Accelerateë¡œ ë¶„ì‚° í•™ìŠµ

### ê¸°ë³¸ í•™ìŠµ ë£¨í”„ â†’ Accelerate ì ìš©

```python
from accelerate import Accelerator

accelerator = Accelerator()  # â‘  Accelerator ì´ˆê¸°í™”

model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
optimizer = AdamW(model.parameters(), lr=3e-5)

# â‘¡ í•µì‹¬: prepare()ë¡œ ë¶„ì‚° í•™ìŠµ ì¤€ë¹„
train_dl, eval_dl, model, optimizer = accelerator.prepare(
    train_dataloader, eval_dataloader, model, optimizer
)

# ì´í›„ ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì •
num_epochs = 3
num_training_steps = num_epochs * len(train_dl)
lr_scheduler = get_scheduler(...)

# í•™ìŠµ ë£¨í”„ (ë³€ê²½ì  í‘œì‹œ)
model.train()
for epoch in range(num_epochs):
    for batch in train_dl:
        # batch = {k: v.to(device) ...} â† ì‚­ì œ! Accelerateê°€ ì²˜ë¦¬
        outputs = model(**batch)
        loss = outputs.loss
        accelerator.backward(loss)  # â‘¢ loss.backward() ëŒ€ì‹ 

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
```

### ë³€ê²½ ì‚¬í•­ ìš”ì•½

| ê¸°ì¡´ ì½”ë“œ | Accelerate ì ìš© | ì´ìœ  |
|-----------|----------------|------|
| `model.to(device)` | ì‚­ì œ | `prepare()`ê°€ ìë™ ì²˜ë¦¬ |
| `batch.to(device)` | ì‚­ì œ | `prepare()`ê°€ ìë™ ì²˜ë¦¬ |
| `loss.backward()` | `accelerator.backward(loss)` | ë¶„ì‚° í™˜ê²½ì—ì„œ gradient ë™ê¸°í™” |
| - | `accelerator.prepare(...)` | ëª¨ë“  ê°ì²´ë¥¼ ë¶„ì‚° í•™ìŠµìš©ìœ¼ë¡œ ë˜í•‘ |

### ì‹¤í–‰ ë°©ë²•

```bash
# 1. ë¶„ì‚° í™˜ê²½ ì„¤ì •
accelerate config

# 2. í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
accelerate launch train.py
```

---

## í•µì‹¬ ê°œë… ì •ë¦¬

### í•™ìŠµ ë£¨í”„ í•„ìˆ˜ ìˆœì„œ
```
Forward Pass â†’ Loss ê³„ì‚° â†’ Backward Pass â†’ Optimizer Step â†’ Scheduler Step â†’ Zero Grad
```

### í‰ê°€ ì‹œ í•„ìˆ˜ ì„¤ì •
```python
model.eval()           # í‰ê°€ ëª¨ë“œ
with torch.no_grad():  # Gradient ê³„ì‚° OFF
```

### ì£¼ìš” ì»´í¬ë„ŒíŠ¸ ì—­í• 

| ì»´í¬ë„ŒíŠ¸ | ì—­í•  |
|----------|------|
| **DataLoader** | ë°ì´í„°ë¥¼ ë°°ì¹˜ ë‹¨ìœ„ë¡œ ê³µê¸‰ |
| **Model** | ì…ë ¥ â†’ ì˜ˆì¸¡ (+ labels ìˆìœ¼ë©´ loss ê³„ì‚°) |
| **Optimizer** | Gradient ê¸°ë°˜ ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸ |
| **Scheduler** | Learning Rate ì ì§„ì  ì¡°ì • |
| **Metric** | ëª¨ë¸ ì„±ëŠ¥ ì¸¡ì • (Accuracy, F1 ë“±) |

### Trainer API vs ì§ì ‘ êµ¬í˜„ ë¹„êµ

| í•­ëª© | Trainer API | ì§ì ‘ êµ¬í˜„ |
|------|-------------|----------|
| ì½”ë“œëŸ‰ | ì ìŒ | ë§ìŒ |
| ìœ ì—°ì„± | ì œí•œì  | ì™„ì „í•œ ì œì–´ |
| ì»¤ìŠ¤í…€ ë¡œì§ | ì½œë°±ìœ¼ë¡œ ì œí•œ | ììœ ë¡­ê²Œ ì¶”ê°€ |
| ë””ë²„ê¹… | ë¸”ë™ë°•ìŠ¤ | ëª¨ë“  ê³¼ì • í™•ì¸ ê°€ëŠ¥ |
| í•™ìŠµ ëª©ì  | â–³ | âœ“ (ë™ì‘ ì›ë¦¬ ì´í•´) |

---

## ë‹¤ìŒ í•™ìŠµ ì¶”ì²œ

1. **Learning Curves** (Chapter 3.5): í•™ìŠµ ê³¡ì„ ìœ¼ë¡œ ê³¼ì í•©/ê³¼ì†Œì í•© ì§„ë‹¨
2. **Mixed Precision Training**: `torch.cuda.amp`ë¡œ ë©”ëª¨ë¦¬ ì ˆì•½ + ì†ë„ í–¥ìƒ
3. **Gradient Accumulation**: ì‘ì€ ë°°ì¹˜ë¡œ í° ë°°ì¹˜ íš¨ê³¼ ë‚´ê¸°
4. **Gradient Clipping**: `clip_grad_norm_`ìœ¼ë¡œ í•™ìŠµ ì•ˆì •í™”

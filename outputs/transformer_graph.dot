digraph transformer {
    rankdir=TB;
    node [shape=box, style="rounded,filled", fontname="Arial"];
    edge [fontname="Arial"];
    label="Transformer 아키텍처";
    labelloc=t;
    fontsize=20;

    subgraph cluster_transformer_architecture {
        label="Transformer 구조";
        style=rounded;
        bgcolor="#fafafa";
        transformer_positional_encoding [label="Positional Encoding", fillcolor="#5F27CD", fontcolor="white"];
        transformer_feed_forward [label="Feed-Forward Network", fillcolor="#5F27CD", fontcolor="white"];
        transformer_transformer_encoder [label="Transformer Encoder", fillcolor="#5F27CD", fontcolor="white"];
        transformer_transformer_decoder [label="Transformer Decoder", fillcolor="#5F27CD", fontcolor="white"];
        transformer_encoder_decoder [label="Encoder-Decoder 구조", fillcolor="#5F27CD", fontcolor="white"];
    }

    subgraph cluster_transformer_training {
        label="학습 기법";
        style=rounded;
        bgcolor="#fafafa";
        transformer_transformer_training [label="Transformer 학습 기법", fillcolor="#54A0FF", fontcolor="white"];
    }

    subgraph cluster_transformer_efficiency {
        label="효율성";
        style=rounded;
        bgcolor="#fafafa";
        transformer_attention_complexity [label="Attention 복잡도", fillcolor="#00D2D3", fontcolor="white"];
        transformer_efficient_attention [label="효율적인 Attention 변형", fillcolor="#00D2D3", fontcolor="white"];
    }

    subgraph cluster_transformer_attention {
        label="Attention Mechanism";
        style=rounded;
        bgcolor="#fafafa";
        transformer_attention_intuition [label="Attention 직관", fillcolor="#FF9F43", fontcolor="white"];
        transformer_attention_score [label="Attention Score 계산", fillcolor="#FF9F43", fontcolor="white"];
        transformer_query_key_value [label="Query, Key, Value", fillcolor="#FF9F43", fontcolor="white"];
        transformer_scaled_dot_product [label="Scaled Dot-Product Attention", fillcolor="#FF9F43", fontcolor="white"];
        transformer_multi_head_attention [label="Multi-Head Attention", fillcolor="#FF9F43", fontcolor="white"];
        transformer_self_attention [label="Self-Attention", fillcolor="#FF9F43", fontcolor="white"];
        transformer_cross_attention [label="Cross-Attention", fillcolor="#FF9F43", fontcolor="white"];
        transformer_causal_attention [label="Causal (Masked) Attention", fillcolor="#FF9F43", fontcolor="white"];
    }

    // External dependencies
    math_foundations_cosine_similarity [label="코사인 유사도\n(수학적 기초)", fillcolor="#cccccc", style="rounded,filled,dashed"];
    deep_learning_layer_normalization [label="Layer Normalization\n(딥러닝 기초)", fillcolor="#cccccc", style="rounded,filled,dashed"];
    deep_learning_softmax [label="Softmax\n(딥러닝 기초)", fillcolor="#cccccc", style="rounded,filled,dashed"];
    deep_learning_seq2seq [label="Sequence-to-Sequence\n(딥러닝 기초)", fillcolor="#cccccc", style="rounded,filled,dashed"];
    deep_learning_mlp [label="다층 퍼셉트론 (MLP)\n(딥러닝 기초)", fillcolor="#cccccc", style="rounded,filled,dashed"];
    deep_learning_residual_connection [label="잔차 연결 (Residual Connection)\n(딥러닝 기초)", fillcolor="#cccccc", style="rounded,filled,dashed"];
    math_foundations_linear_transformation [label="선형 변환\n(수학적 기초)", fillcolor="#cccccc", style="rounded,filled,dashed"];
    deep_learning_adamw [label="AdamW\n(딥러닝 기초)", fillcolor="#cccccc", style="rounded,filled,dashed"];
    math_foundations_dot_product [label="내적 (Dot Product)\n(수학적 기초)", fillcolor="#cccccc", style="rounded,filled,dashed"];

    // Edges
    deep_learning_seq2seq -> transformer_attention_intuition [style=dashed, color="#999999"];
    transformer_attention_intuition -> transformer_attention_score;
    math_foundations_dot_product -> transformer_attention_score [style=dashed, color="#999999"];
    math_foundations_cosine_similarity -> transformer_attention_score [style=dashed, color="#999999"];
    transformer_attention_intuition -> transformer_query_key_value;
    math_foundations_linear_transformation -> transformer_query_key_value [style=dashed, color="#999999"];
    transformer_attention_score -> transformer_scaled_dot_product;
    transformer_query_key_value -> transformer_scaled_dot_product;
    deep_learning_softmax -> transformer_scaled_dot_product [style=dashed, color="#999999"];
    transformer_scaled_dot_product -> transformer_multi_head_attention;
    transformer_multi_head_attention -> transformer_self_attention;
    transformer_multi_head_attention -> transformer_cross_attention;
    transformer_self_attention -> transformer_causal_attention;
    transformer_attention_intuition -> transformer_positional_encoding;
    deep_learning_mlp -> transformer_feed_forward [style=dashed, color="#999999"];
    transformer_self_attention -> transformer_transformer_encoder;
    transformer_feed_forward -> transformer_transformer_encoder;
    transformer_positional_encoding -> transformer_transformer_encoder;
    deep_learning_layer_normalization -> transformer_transformer_encoder [style=dashed, color="#999999"];
    deep_learning_residual_connection -> transformer_transformer_encoder [style=dashed, color="#999999"];
    transformer_causal_attention -> transformer_transformer_decoder;
    transformer_cross_attention -> transformer_transformer_decoder;
    transformer_feed_forward -> transformer_transformer_decoder;
    deep_learning_layer_normalization -> transformer_transformer_decoder [style=dashed, color="#999999"];
    deep_learning_residual_connection -> transformer_transformer_decoder [style=dashed, color="#999999"];
    transformer_transformer_encoder -> transformer_encoder_decoder;
    transformer_transformer_decoder -> transformer_encoder_decoder;
    transformer_self_attention -> transformer_attention_complexity;
    transformer_attention_complexity -> transformer_efficient_attention;
    transformer_transformer_encoder -> transformer_transformer_training;
    transformer_transformer_decoder -> transformer_transformer_training;
    deep_learning_adamw -> transformer_transformer_training [style=dashed, color="#999999"];
}
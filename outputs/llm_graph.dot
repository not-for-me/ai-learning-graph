digraph llm {
    rankdir=TB;
    node [shape=box, style="rounded,filled", fontname="Arial"];
    edge [fontname="Arial"];
    label="Large Language Models";
    labelloc=t;
    fontsize=20;

    subgraph cluster_llm_training {
        label="학습";
        style=rounded;
        bgcolor="#fafafa";
        llm_pretraining [label="사전학습 (Pretraining)", fillcolor="#0984E3", fontcolor="white"];
        llm_next_token_prediction [label="Next Token Prediction", fillcolor="#0984E3", fontcolor="white"];
        llm_masked_language_modeling [label="Masked Language Modeling (MLM)", fillcolor="#0984E3", fontcolor="white"];
        llm_scaling_laws [label="스케일링 법칙", fillcolor="#0984E3", fontcolor="white"];
        llm_chinchilla_scaling [label="Chinchilla 스케일링", fillcolor="#0984E3", fontcolor="white"];
    }

    subgraph cluster_llm_alignment {
        label="정렬";
        style=rounded;
        bgcolor="#fafafa";
        llm_rlhf [label="RLHF", fillcolor="#E84393", fontcolor="white"];
        llm_reward_model [label="Reward Model", fillcolor="#E84393", fontcolor="white"];
        llm_ppo [label="PPO (Proximal Policy Optimization)", fillcolor="#E84393", fontcolor="white"];
        llm_dpo [label="DPO (Direct Preference Optimization)", fillcolor="#E84393", fontcolor="white"];
    }

    subgraph cluster_llm_finetuning {
        label="Fine-tuning";
        style=rounded;
        bgcolor="#fafafa";
        llm_finetuning [label="Fine-tuning 기초", fillcolor="#FDCB6E", fontcolor="white"];
        llm_instruction_tuning [label="Instruction Tuning", fillcolor="#FDCB6E", fontcolor="white"];
        llm_chat_format [label="Chat Format & System Prompt", fillcolor="#FDCB6E", fontcolor="white"];
        llm_peft [label="PEFT (Parameter-Efficient Fine-Tuning)", fillcolor="#FDCB6E", fontcolor="white"];
        llm_lora [label="LoRA", fillcolor="#FDCB6E", fontcolor="white"];
        llm_qlora [label="QLoRA", fillcolor="#FDCB6E", fontcolor="white"];
    }

    subgraph cluster_llm_preprocessing {
        label="전처리";
        style=rounded;
        bgcolor="#fafafa";
        llm_tokenization [label="토크나이제이션", fillcolor="#00B894", fontcolor="white"];
        llm_bpe [label="BPE (Byte Pair Encoding)", fillcolor="#00B894", fontcolor="white"];
        llm_embedding_layer [label="임베딩 레이어", fillcolor="#00B894", fontcolor="white"];
        llm_rotary_embedding [label="RoPE (Rotary Position Embedding)", fillcolor="#00B894", fontcolor="white"];
    }

    subgraph cluster_llm_architecture {
        label="아키텍처";
        style=rounded;
        bgcolor="#fafafa";
        llm_gpt_architecture [label="GPT 아키텍처", fillcolor="#E17055", fontcolor="white"];
        llm_bert_architecture [label="BERT 아키텍처", fillcolor="#E17055", fontcolor="white"];
        llm_t5_architecture [label="T5 아키텍처", fillcolor="#E17055", fontcolor="white"];
    }

    subgraph cluster_llm_prompting {
        label="프롬프팅";
        style=rounded;
        bgcolor="#fafafa";
        llm_prompting_techniques [label="프롬프팅 기법", fillcolor="#00CEC9", fontcolor="white"];
        llm_few_shot_prompting [label="Few-shot Prompting", fillcolor="#00CEC9", fontcolor="white"];
        llm_chain_of_thought [label="Chain-of-Thought (CoT)", fillcolor="#00CEC9", fontcolor="white"];
    }

    subgraph cluster_llm_inference {
        label="추론";
        style=rounded;
        bgcolor="#fafafa";
        llm_autoregressive_generation [label="자기회귀 생성", fillcolor="#6C5CE7", fontcolor="white"];
        llm_sampling_strategies [label="샘플링 전략", fillcolor="#6C5CE7", fontcolor="white"];
        llm_temperature_sampling [label="Temperature 샘플링", fillcolor="#6C5CE7", fontcolor="white"];
        llm_top_k_top_p [label="Top-k / Top-p (Nucleus) Sampling", fillcolor="#6C5CE7", fontcolor="white"];
        llm_kv_cache [label="KV Cache", fillcolor="#6C5CE7", fontcolor="white"];
        llm_flash_attention [label="Flash Attention", fillcolor="#6C5CE7", fontcolor="white"];
    }

    subgraph cluster_llm_optimization {
        label="최적화";
        style=rounded;
        bgcolor="#fafafa";
        llm_quantization [label="양자화 (Quantization)", fillcolor="#636E72", fontcolor="white"];
    }

    subgraph cluster_llm_context {
        label="컨텍스트";
        style=rounded;
        bgcolor="#fafafa";
        llm_context_length_extension [label="Context Length 확장", fillcolor="#A29BFE", fontcolor="white"];
        llm_sentence_embeddings [label="문장/문서 임베딩", fillcolor="#A29BFE", fontcolor="white"];
    }

    // External dependencies
    transformer_transformer_decoder [label="Transformer Decoder\n(Transformer 아키텍처)", fillcolor="#cccccc", style="rounded,filled,dashed"];
    transformer_transformer_training [label="Transformer 학습 기법\n(Transformer 아키텍처)", fillcolor="#cccccc", style="rounded,filled,dashed"];
    math_foundations_vector [label="벡터\n(수학적 기초)", fillcolor="#cccccc", style="rounded,filled,dashed"];
    deep_learning_softmax [label="Softmax\n(딥러닝 기초)", fillcolor="#cccccc", style="rounded,filled,dashed"];
    transformer_attention_complexity [label="Attention 복잡도\n(Transformer 아키텍처)", fillcolor="#cccccc", style="rounded,filled,dashed"];
    transformer_transformer_encoder [label="Transformer Encoder\n(Transformer 아키텍처)", fillcolor="#cccccc", style="rounded,filled,dashed"];
    transformer_encoder_decoder [label="Encoder-Decoder 구조\n(Transformer 아키텍처)", fillcolor="#cccccc", style="rounded,filled,dashed"];
    transformer_causal_attention [label="Causal (Masked) Attention\n(Transformer 아키텍처)", fillcolor="#cccccc", style="rounded,filled,dashed"];
    deep_learning_cross_entropy [label="Cross-Entropy Loss\n(딥러닝 기초)", fillcolor="#cccccc", style="rounded,filled,dashed"];
    transformer_positional_encoding [label="Positional Encoding\n(Transformer 아키텍처)", fillcolor="#cccccc", style="rounded,filled,dashed"];
    transformer_efficient_attention [label="효율적인 Attention 변형\n(Transformer 아키텍처)", fillcolor="#cccccc", style="rounded,filled,dashed"];

    // Edges
    transformer_transformer_decoder -> llm_gpt_architecture [style=dashed, color="#999999"];
    transformer_causal_attention -> llm_gpt_architecture [style=dashed, color="#999999"];
    transformer_transformer_encoder -> llm_bert_architecture [style=dashed, color="#999999"];
    transformer_encoder_decoder -> llm_t5_architecture [style=dashed, color="#999999"];
    llm_tokenization -> llm_bpe;
    llm_bpe -> llm_embedding_layer;
    math_foundations_vector -> llm_embedding_layer [style=dashed, color="#999999"];
    llm_embedding_layer -> llm_rotary_embedding;
    transformer_positional_encoding -> llm_rotary_embedding [style=dashed, color="#999999"];
    llm_gpt_architecture -> llm_pretraining;
    transformer_transformer_training -> llm_pretraining [style=dashed, color="#999999"];
    llm_pretraining -> llm_next_token_prediction;
    deep_learning_cross_entropy -> llm_next_token_prediction [style=dashed, color="#999999"];
    llm_bert_architecture -> llm_masked_language_modeling;
    llm_pretraining -> llm_masked_language_modeling;
    llm_pretraining -> llm_scaling_laws;
    llm_scaling_laws -> llm_chinchilla_scaling;
    llm_next_token_prediction -> llm_autoregressive_generation;
    transformer_causal_attention -> llm_autoregressive_generation [style=dashed, color="#999999"];
    llm_autoregressive_generation -> llm_sampling_strategies;
    deep_learning_softmax -> llm_sampling_strategies [style=dashed, color="#999999"];
    llm_sampling_strategies -> llm_temperature_sampling;
    llm_temperature_sampling -> llm_top_k_top_p;
    llm_autoregressive_generation -> llm_kv_cache;
    transformer_attention_complexity -> llm_kv_cache [style=dashed, color="#999999"];
    llm_kv_cache -> llm_flash_attention;
    transformer_efficient_attention -> llm_flash_attention [style=dashed, color="#999999"];
    llm_pretraining -> llm_finetuning;
    llm_finetuning -> llm_instruction_tuning;
    llm_instruction_tuning -> llm_chat_format;
    llm_instruction_tuning -> llm_rlhf;
    llm_rlhf -> llm_reward_model;
    llm_reward_model -> llm_ppo;
    llm_rlhf -> llm_dpo;
    llm_finetuning -> llm_peft;
    llm_peft -> llm_lora;
    llm_lora -> llm_qlora;
    llm_quantization -> llm_qlora;
    llm_gpt_architecture -> llm_quantization;
    llm_chat_format -> llm_prompting_techniques;
    llm_prompting_techniques -> llm_few_shot_prompting;
    llm_few_shot_prompting -> llm_chain_of_thought;
    llm_rotary_embedding -> llm_context_length_extension;
    llm_kv_cache -> llm_context_length_extension;
    llm_embedding_layer -> llm_sentence_embeddings;
    llm_bert_architecture -> llm_sentence_embeddings;
}
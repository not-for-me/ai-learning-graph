digraph LearningGraph {
    rankdir=TB;
    node [shape=box, style="rounded,filled", fontname="Arial"];
    edge [fontname="Arial"];
    compound=true;

    subgraph cluster_math_foundations {
        label="수학적 기초";
        style=rounded;
        bgcolor="#f5f5f5";

        math_foundations_vector [label="벡터", fillcolor="#4A90D9", fontcolor="white"];
        math_foundations_dot_product [label="내적 (Dot Product)", fillcolor="#4A90D9", fontcolor="white"];
        math_foundations_vector_norm [label="벡터 노름 (Norm)", fillcolor="#4A90D9", fontcolor="white"];
        math_foundations_matrix [label="행렬 (Matrix)", fillcolor="#4A90D9", fontcolor="white"];
        math_foundations_matrix_multiplication [label="행렬 곱셈", fillcolor="#4A90D9", fontcolor="white"];
        math_foundations_transpose [label="전치 (Transpose)", fillcolor="#4A90D9", fontcolor="white"];
        math_foundations_linear_transformation [label="선형 변환", fillcolor="#4A90D9", fontcolor="white"];
        math_foundations_eigenvalue [label="고유값/고유벡터", fillcolor="#4A90D9", fontcolor="white"];
        math_foundations_derivative [label="미분 (Derivative)", fillcolor="#D94A4A", fontcolor="white"];
        math_foundations_partial_derivative [label="편미분", fillcolor="#D94A4A", fontcolor="white"];
        math_foundations_chain_rule [label="연쇄 법칙 (Chain Rule)", fillcolor="#D94A4A", fontcolor="white"];
        math_foundations_gradient [label="그래디언트 (Gradient)", fillcolor="#D94A4A", fontcolor="white"];
        math_foundations_probability_basics [label="확률 기초", fillcolor="#4AD94A", fontcolor="white"];
        math_foundations_conditional_probability [label="조건부 확률", fillcolor="#4AD94A", fontcolor="white"];
        math_foundations_bayes_theorem [label="베이즈 정리", fillcolor="#4AD94A", fontcolor="white"];
        math_foundations_expectation [label="기댓값", fillcolor="#4AD94A", fontcolor="white"];
        math_foundations_variance [label="분산과 표준편차", fillcolor="#4AD94A", fontcolor="white"];
        math_foundations_maximum_likelihood [label="최대 우도 추정 (MLE)", fillcolor="#4AD94A", fontcolor="white"];
        math_foundations_cosine_similarity [label="코사인 유사도", fillcolor="#4A90D9", fontcolor="white"];
    }

    subgraph cluster_deep_learning {
        label="딥러닝 기초";
        style=rounded;
        bgcolor="#f5f5f5";

        deep_learning_perceptron [label="퍼셉트론", fillcolor="#FF6B6B", fontcolor="white"];
        deep_learning_activation_function [label="활성화 함수", fillcolor="#FF6B6B", fontcolor="white"];
        deep_learning_relu [label="ReLU", fillcolor="#FF6B6B", fontcolor="white"];
        deep_learning_sigmoid [label="Sigmoid", fillcolor="#FF6B6B", fontcolor="white"];
        deep_learning_softmax [label="Softmax", fillcolor="#FF6B6B", fontcolor="white"];
        deep_learning_neural_network_layer [label="신경망 층 (Layer)", fillcolor="#FF6B6B", fontcolor="white"];
        deep_learning_mlp [label="다층 퍼셉트론 (MLP)", fillcolor="#FF6B6B", fontcolor="white"];
        deep_learning_loss_function [label="손실 함수", fillcolor="#4ECDC4", fontcolor="white"];
        deep_learning_cross_entropy [label="Cross-Entropy Loss", fillcolor="#4ECDC4", fontcolor="white"];
        deep_learning_gradient_descent [label="경사하강법", fillcolor="#4ECDC4", fontcolor="white"];
        deep_learning_learning_rate [label="학습률 (Learning Rate)", fillcolor="#4ECDC4", fontcolor="white"];
        deep_learning_sgd [label="Stochastic Gradient Descent", fillcolor="#4ECDC4", fontcolor="white"];
        deep_learning_momentum [label="Momentum", fillcolor="#4ECDC4", fontcolor="white"];
        deep_learning_adam [label="Adam Optimizer", fillcolor="#4ECDC4", fontcolor="white"];
        deep_learning_adamw [label="AdamW", fillcolor="#4ECDC4", fontcolor="white"];
        deep_learning_backpropagation [label="역전파 (Backpropagation)", fillcolor="#4ECDC4", fontcolor="white"];
        deep_learning_computational_graph [label="계산 그래프", fillcolor="#4ECDC4", fontcolor="white"];
        deep_learning_overfitting [label="과적합 (Overfitting)", fillcolor="#45B7D1", fontcolor="white"];
        deep_learning_regularization [label="정규화 (Regularization)", fillcolor="#45B7D1", fontcolor="white"];
        deep_learning_dropout [label="Dropout", fillcolor="#45B7D1", fontcolor="white"];
        deep_learning_batch_normalization [label="Batch Normalization", fillcolor="#45B7D1", fontcolor="white"];
        deep_learning_layer_normalization [label="Layer Normalization", fillcolor="#45B7D1", fontcolor="white"];
        deep_learning_vanishing_gradient [label="기울기 소실 문제", fillcolor="#96CEB4", fontcolor="white"];
        deep_learning_residual_connection [label="잔차 연결 (Residual Connection)", fillcolor="#96CEB4", fontcolor="white"];
        deep_learning_convolution [label="합성곱 (Convolution)", fillcolor="#FFEAA7", fontcolor="white"];
        deep_learning_pooling [label="Pooling", fillcolor="#FFEAA7", fontcolor="white"];
        deep_learning_cnn_architecture [label="CNN 아키텍처", fillcolor="#FFEAA7", fontcolor="white"];
        deep_learning_resnet [label="ResNet", fillcolor="#FFEAA7", fontcolor="white"];
        deep_learning_rnn [label="순환 신경망 (RNN)", fillcolor="#DDA0DD", fontcolor="white"];
        deep_learning_lstm [label="LSTM", fillcolor="#DDA0DD", fontcolor="white"];
        deep_learning_seq2seq [label="Sequence-to-Sequence", fillcolor="#DDA0DD", fontcolor="white"];
    }

    subgraph cluster_transformer {
        label="Transformer 아키텍처";
        style=rounded;
        bgcolor="#f5f5f5";

        transformer_attention_intuition [label="Attention 직관", fillcolor="#FF9F43", fontcolor="white"];
        transformer_attention_score [label="Attention Score 계산", fillcolor="#FF9F43", fontcolor="white"];
        transformer_query_key_value [label="Query, Key, Value", fillcolor="#FF9F43", fontcolor="white"];
        transformer_scaled_dot_product [label="Scaled Dot-Product Attention", fillcolor="#FF9F43", fontcolor="white"];
        transformer_multi_head_attention [label="Multi-Head Attention", fillcolor="#FF9F43", fontcolor="white"];
        transformer_self_attention [label="Self-Attention", fillcolor="#FF9F43", fontcolor="white"];
        transformer_cross_attention [label="Cross-Attention", fillcolor="#FF9F43", fontcolor="white"];
        transformer_causal_attention [label="Causal (Masked) Attention", fillcolor="#FF9F43", fontcolor="white"];
        transformer_positional_encoding [label="Positional Encoding", fillcolor="#5F27CD", fontcolor="white"];
        transformer_feed_forward [label="Feed-Forward Network", fillcolor="#5F27CD", fontcolor="white"];
        transformer_transformer_encoder [label="Transformer Encoder", fillcolor="#5F27CD", fontcolor="white"];
        transformer_transformer_decoder [label="Transformer Decoder", fillcolor="#5F27CD", fontcolor="white"];
        transformer_encoder_decoder [label="Encoder-Decoder 구조", fillcolor="#5F27CD", fontcolor="white"];
        transformer_attention_complexity [label="Attention 복잡도", fillcolor="#00D2D3", fontcolor="white"];
        transformer_efficient_attention [label="효율적인 Attention 변형", fillcolor="#00D2D3", fontcolor="white"];
        transformer_transformer_training [label="Transformer 학습 기법", fillcolor="#54A0FF", fontcolor="white"];
    }

    subgraph cluster_llm {
        label="Large Language Models";
        style=rounded;
        bgcolor="#f5f5f5";

        llm_gpt_architecture [label="GPT 아키텍처", fillcolor="#E17055", fontcolor="white"];
        llm_bert_architecture [label="BERT 아키텍처", fillcolor="#E17055", fontcolor="white"];
        llm_t5_architecture [label="T5 아키텍처", fillcolor="#E17055", fontcolor="white"];
        llm_tokenization [label="토크나이제이션", fillcolor="#00B894", fontcolor="white"];
        llm_bpe [label="BPE (Byte Pair Encoding)", fillcolor="#00B894", fontcolor="white"];
        llm_embedding_layer [label="임베딩 레이어", fillcolor="#00B894", fontcolor="white"];
        llm_rotary_embedding [label="RoPE (Rotary Position Embedding)", fillcolor="#00B894", fontcolor="white"];
        llm_pretraining [label="사전학습 (Pretraining)", fillcolor="#0984E3", fontcolor="white"];
        llm_next_token_prediction [label="Next Token Prediction", fillcolor="#0984E3", fontcolor="white"];
        llm_masked_language_modeling [label="Masked Language Modeling (MLM)", fillcolor="#0984E3", fontcolor="white"];
        llm_scaling_laws [label="스케일링 법칙", fillcolor="#0984E3", fontcolor="white"];
        llm_chinchilla_scaling [label="Chinchilla 스케일링", fillcolor="#0984E3", fontcolor="white"];
        llm_autoregressive_generation [label="자기회귀 생성", fillcolor="#6C5CE7", fontcolor="white"];
        llm_sampling_strategies [label="샘플링 전략", fillcolor="#6C5CE7", fontcolor="white"];
        llm_temperature_sampling [label="Temperature 샘플링", fillcolor="#6C5CE7", fontcolor="white"];
        llm_top_k_top_p [label="Top-k / Top-p (Nucleus) Sampling", fillcolor="#6C5CE7", fontcolor="white"];
        llm_kv_cache [label="KV Cache", fillcolor="#6C5CE7", fontcolor="white"];
        llm_flash_attention [label="Flash Attention", fillcolor="#6C5CE7", fontcolor="white"];
        llm_finetuning [label="Fine-tuning 기초", fillcolor="#FDCB6E", fontcolor="white"];
        llm_instruction_tuning [label="Instruction Tuning", fillcolor="#FDCB6E", fontcolor="white"];
        llm_chat_format [label="Chat Format & System Prompt", fillcolor="#FDCB6E", fontcolor="white"];
        llm_rlhf [label="RLHF", fillcolor="#E84393", fontcolor="white"];
        llm_reward_model [label="Reward Model", fillcolor="#E84393", fontcolor="white"];
        llm_ppo [label="PPO (Proximal Policy Optimization)", fillcolor="#E84393", fontcolor="white"];
        llm_dpo [label="DPO (Direct Preference Optimization)", fillcolor="#E84393", fontcolor="white"];
        llm_peft [label="PEFT (Parameter-Efficient Fine-Tuning)", fillcolor="#FDCB6E", fontcolor="white"];
        llm_lora [label="LoRA", fillcolor="#FDCB6E", fontcolor="white"];
        llm_qlora [label="QLoRA", fillcolor="#FDCB6E", fontcolor="white"];
        llm_quantization [label="양자화 (Quantization)", fillcolor="#636E72", fontcolor="white"];
        llm_prompting_techniques [label="프롬프팅 기법", fillcolor="#00CEC9", fontcolor="white"];
        llm_few_shot_prompting [label="Few-shot Prompting", fillcolor="#00CEC9", fontcolor="white"];
        llm_chain_of_thought [label="Chain-of-Thought (CoT)", fillcolor="#00CEC9", fontcolor="white"];
        llm_context_length_extension [label="Context Length 확장", fillcolor="#A29BFE", fontcolor="white"];
        llm_sentence_embeddings [label="문장/문서 임베딩", fillcolor="#A29BFE", fontcolor="white"];
    }

    subgraph cluster_agent {
        label="AI Agents";
        style=rounded;
        bgcolor="#f5f5f5";

        agent_agent_fundamentals [label="Agent 기초 개념", fillcolor="#FF6B6B", fontcolor="white"];
        agent_agent_loop [label="Agent Loop (Perception-Action)", fillcolor="#FF6B6B", fontcolor="white"];
        agent_tool_use [label="Tool Use (Function Calling)", fillcolor="#4ECDC4", fontcolor="white"];
        agent_function_calling_format [label="Function Calling 포맷", fillcolor="#4ECDC4", fontcolor="white"];
        agent_tool_selection [label="Tool Selection", fillcolor="#4ECDC4", fontcolor="white"];
        agent_multi_tool_orchestration [label="Multi-Tool Orchestration", fillcolor="#4ECDC4", fontcolor="white"];
        agent_code_execution [label="Code Execution (Code Interpreter)", fillcolor="#4ECDC4", fontcolor="white"];
        agent_rag [label="RAG 개요", fillcolor="#45B7D1", fontcolor="white"];
        agent_semantic_search [label="시맨틱 검색", fillcolor="#45B7D1", fontcolor="white"];
        agent_vector_database [label="Vector Database", fillcolor="#45B7D1", fontcolor="white"];
        agent_chunking_strategies [label="Chunking 전략", fillcolor="#45B7D1", fontcolor="white"];
        agent_hybrid_search [label="Hybrid Search", fillcolor="#45B7D1", fontcolor="white"];
        agent_advanced_rag [label="Advanced RAG 기법", fillcolor="#45B7D1", fontcolor="white"];
        agent_agentic_rag [label="Agentic RAG", fillcolor="#45B7D1", fontcolor="white"];
        agent_reasoning_techniques [label="추론 기법", fillcolor="#96CEB4", fontcolor="white"];
        agent_react [label="ReAct (Reasoning + Acting)", fillcolor="#96CEB4", fontcolor="white"];
        agent_self_reflection [label="Self-Reflection", fillcolor="#96CEB4", fontcolor="white"];
        agent_reflexion [label="Reflexion", fillcolor="#96CEB4", fontcolor="white"];
        agent_planning_strategies [label="Planning 전략", fillcolor="#96CEB4", fontcolor="white"];
        agent_hierarchical_planning [label="Hierarchical Planning", fillcolor="#96CEB4", fontcolor="white"];
        agent_agent_memory [label="Agent Memory", fillcolor="#DDA0DD", fontcolor="white"];
        agent_short_term_memory [label="Short-term Memory", fillcolor="#DDA0DD", fontcolor="white"];
        agent_long_term_memory [label="Long-term Memory", fillcolor="#DDA0DD", fontcolor="white"];
        agent_memory_management [label="Memory Management", fillcolor="#DDA0DD", fontcolor="white"];
        agent_multi_agent_basics [label="Multi-Agent 기초", fillcolor="#FFEAA7", fontcolor="white"];
        agent_agent_communication [label="Agent 간 통신", fillcolor="#FFEAA7", fontcolor="white"];
        agent_agent_roles [label="Agent 역할 설계", fillcolor="#FFEAA7", fontcolor="white"];
        agent_coordination_patterns [label="조율 패턴", fillcolor="#FFEAA7", fontcolor="white"];
        agent_agent_frameworks [label="Agent Framework", fillcolor="#74B9FF", fontcolor="white"];
        agent_langchain_basics [label="LangChain / LangGraph", fillcolor="#74B9FF", fontcolor="white"];
        agent_autogen_basics [label="AutoGen", fillcolor="#74B9FF", fontcolor="white"];
        agent_complex_workflows [label="복잡한 워크플로우 구축", fillcolor="#74B9FF", fontcolor="white"];
        agent_data_analysis_agent [label="Data Analysis Agent", fillcolor="#A29BFE", fontcolor="white"];
        agent_research_agent [label="Research Agent", fillcolor="#A29BFE", fontcolor="white"];
        agent_production_deployment [label="프로덕션 배포", fillcolor="#A29BFE", fontcolor="white"];
    }

    subgraph cluster_practical_tools {
        label="실무 도구";
        style=rounded;
        bgcolor="#f5f5f5";

        practical_tools_hf_transformers_intro [label="HuggingFace Transformers 소개", fillcolor="#FFD21E", fontcolor="white"];
        practical_tools_hf_pipeline [label="Pipeline API", fillcolor="#FFD21E", fontcolor="white"];
        practical_tools_hf_tokenizer [label="Tokenizer 사용법", fillcolor="#FFD21E", fontcolor="white"];
        practical_tools_hf_model_loading [label="모델 로드 및 추론", fillcolor="#FFD21E", fontcolor="white"];
        practical_tools_hf_embeddings [label="임베딩 추출 및 유사도", fillcolor="#FFD21E", fontcolor="white"];
        practical_tools_hf_datasets [label="Datasets 라이브러리", fillcolor="#FFD21E", fontcolor="white"];
        practical_tools_hf_data_collator [label="DataCollator와 동적 패딩", fillcolor="#FFD21E", fontcolor="white"];
        practical_tools_hf_dataloader [label="PyTorch DataLoader 통합", fillcolor="#FFD21E", fontcolor="white"];
        practical_tools_hf_training_loop [label="Full Training Loop 구현", fillcolor="#FFD21E", fontcolor="white"];
        practical_tools_hf_optimizer_scheduler [label="Optimizer와 Scheduler 설정", fillcolor="#FFD21E", fontcolor="white"];
        practical_tools_hf_evaluation [label="평가 루프 구현", fillcolor="#FFD21E", fontcolor="white"];
        practical_tools_hf_trainer_api [label="Trainer API", fillcolor="#FFD21E", fontcolor="white"];
        practical_tools_hf_accelerate [label="Accelerate 분산 학습", fillcolor="#FFD21E", fontcolor="white"];
        practical_tools_encoder_decoder_comparison [label="인코더 vs 디코더 모델 비교", fillcolor="#FF6B6B", fontcolor="white"];
        practical_tools_cross_attention_understanding [label="Cross-Attention 이해", fillcolor="#FF6B6B", fontcolor="white"];
        practical_tools_ollama_intro [label="Ollama 소개", fillcolor="#4ECDC4", fontcolor="white"];
        practical_tools_llamaindex_intro [label="LlamaIndex 소개", fillcolor="#4ECDC4", fontcolor="white"];
    }

    // Edges
    math_foundations_vector -> math_foundations_dot_product;
    math_foundations_vector -> math_foundations_vector_norm;
    math_foundations_vector -> math_foundations_matrix;
    math_foundations_matrix -> math_foundations_matrix_multiplication;
    math_foundations_dot_product -> math_foundations_matrix_multiplication;
    math_foundations_matrix -> math_foundations_transpose;
    math_foundations_matrix_multiplication -> math_foundations_linear_transformation;
    math_foundations_linear_transformation -> math_foundations_eigenvalue;
    math_foundations_derivative -> math_foundations_partial_derivative;
    math_foundations_derivative -> math_foundations_chain_rule;
    math_foundations_partial_derivative -> math_foundations_gradient;
    math_foundations_vector -> math_foundations_gradient;
    math_foundations_probability_basics -> math_foundations_conditional_probability;
    math_foundations_conditional_probability -> math_foundations_bayes_theorem;
    math_foundations_probability_basics -> math_foundations_expectation;
    math_foundations_expectation -> math_foundations_variance;
    math_foundations_bayes_theorem -> math_foundations_maximum_likelihood;
    math_foundations_gradient -> math_foundations_maximum_likelihood;
    math_foundations_dot_product -> math_foundations_cosine_similarity;
    math_foundations_vector_norm -> math_foundations_cosine_similarity;
    math_foundations_linear_transformation -> deep_learning_perceptron [style=dashed, color="#666666"];
    deep_learning_perceptron -> deep_learning_activation_function;
    deep_learning_activation_function -> deep_learning_relu;
    deep_learning_activation_function -> deep_learning_sigmoid;
    math_foundations_probability_basics -> deep_learning_sigmoid [style=dashed, color="#666666"];
    deep_learning_activation_function -> deep_learning_softmax;
    math_foundations_probability_basics -> deep_learning_softmax [style=dashed, color="#666666"];
    deep_learning_perceptron -> deep_learning_neural_network_layer;
    math_foundations_linear_transformation -> deep_learning_neural_network_layer [style=dashed, color="#666666"];
    deep_learning_neural_network_layer -> deep_learning_mlp;
    deep_learning_activation_function -> deep_learning_mlp;
    deep_learning_loss_function -> deep_learning_cross_entropy;
    deep_learning_softmax -> deep_learning_cross_entropy;
    math_foundations_maximum_likelihood -> deep_learning_cross_entropy [style=dashed, color="#666666"];
    deep_learning_loss_function -> deep_learning_gradient_descent;
    math_foundations_gradient -> deep_learning_gradient_descent [style=dashed, color="#666666"];
    deep_learning_gradient_descent -> deep_learning_learning_rate;
    deep_learning_gradient_descent -> deep_learning_sgd;
    deep_learning_sgd -> deep_learning_momentum;
    deep_learning_momentum -> deep_learning_adam;
    deep_learning_learning_rate -> deep_learning_adam;
    deep_learning_adam -> deep_learning_adamw;
    deep_learning_regularization -> deep_learning_adamw;
    deep_learning_gradient_descent -> deep_learning_backpropagation;
    deep_learning_mlp -> deep_learning_backpropagation;
    math_foundations_chain_rule -> deep_learning_backpropagation [style=dashed, color="#666666"];
    deep_learning_backpropagation -> deep_learning_computational_graph;
    deep_learning_overfitting -> deep_learning_regularization;
    math_foundations_vector_norm -> deep_learning_regularization [style=dashed, color="#666666"];
    deep_learning_regularization -> deep_learning_dropout;
    deep_learning_mlp -> deep_learning_dropout;
    deep_learning_mlp -> deep_learning_batch_normalization;
    math_foundations_variance -> deep_learning_batch_normalization [style=dashed, color="#666666"];
    deep_learning_batch_normalization -> deep_learning_layer_normalization;
    deep_learning_backpropagation -> deep_learning_vanishing_gradient;
    deep_learning_sigmoid -> deep_learning_vanishing_gradient;
    deep_learning_vanishing_gradient -> deep_learning_residual_connection;
    deep_learning_neural_network_layer -> deep_learning_convolution;
    deep_learning_convolution -> deep_learning_pooling;
    deep_learning_convolution -> deep_learning_cnn_architecture;
    deep_learning_pooling -> deep_learning_cnn_architecture;
    deep_learning_cnn_architecture -> deep_learning_resnet;
    deep_learning_residual_connection -> deep_learning_resnet;
    deep_learning_mlp -> deep_learning_rnn;
    deep_learning_rnn -> deep_learning_lstm;
    deep_learning_vanishing_gradient -> deep_learning_lstm;
    deep_learning_lstm -> deep_learning_seq2seq;
    deep_learning_seq2seq -> transformer_attention_intuition [style=dashed, color="#666666"];
    transformer_attention_intuition -> transformer_attention_score;
    math_foundations_dot_product -> transformer_attention_score [style=dashed, color="#666666"];
    math_foundations_cosine_similarity -> transformer_attention_score [style=dashed, color="#666666"];
    transformer_attention_intuition -> transformer_query_key_value;
    math_foundations_linear_transformation -> transformer_query_key_value [style=dashed, color="#666666"];
    transformer_attention_score -> transformer_scaled_dot_product;
    transformer_query_key_value -> transformer_scaled_dot_product;
    deep_learning_softmax -> transformer_scaled_dot_product [style=dashed, color="#666666"];
    transformer_scaled_dot_product -> transformer_multi_head_attention;
    transformer_multi_head_attention -> transformer_self_attention;
    transformer_multi_head_attention -> transformer_cross_attention;
    transformer_self_attention -> transformer_causal_attention;
    transformer_attention_intuition -> transformer_positional_encoding;
    deep_learning_mlp -> transformer_feed_forward [style=dashed, color="#666666"];
    transformer_self_attention -> transformer_transformer_encoder;
    transformer_feed_forward -> transformer_transformer_encoder;
    transformer_positional_encoding -> transformer_transformer_encoder;
    deep_learning_layer_normalization -> transformer_transformer_encoder [style=dashed, color="#666666"];
    deep_learning_residual_connection -> transformer_transformer_encoder [style=dashed, color="#666666"];
    transformer_causal_attention -> transformer_transformer_decoder;
    transformer_cross_attention -> transformer_transformer_decoder;
    transformer_feed_forward -> transformer_transformer_decoder;
    deep_learning_layer_normalization -> transformer_transformer_decoder [style=dashed, color="#666666"];
    deep_learning_residual_connection -> transformer_transformer_decoder [style=dashed, color="#666666"];
    transformer_transformer_encoder -> transformer_encoder_decoder;
    transformer_transformer_decoder -> transformer_encoder_decoder;
    transformer_self_attention -> transformer_attention_complexity;
    transformer_attention_complexity -> transformer_efficient_attention;
    transformer_transformer_encoder -> transformer_transformer_training;
    transformer_transformer_decoder -> transformer_transformer_training;
    deep_learning_adamw -> transformer_transformer_training [style=dashed, color="#666666"];
    transformer_transformer_decoder -> llm_gpt_architecture [style=dashed, color="#666666"];
    transformer_causal_attention -> llm_gpt_architecture [style=dashed, color="#666666"];
    transformer_transformer_encoder -> llm_bert_architecture [style=dashed, color="#666666"];
    transformer_encoder_decoder -> llm_t5_architecture [style=dashed, color="#666666"];
    llm_tokenization -> llm_bpe;
    llm_bpe -> llm_embedding_layer;
    math_foundations_vector -> llm_embedding_layer [style=dashed, color="#666666"];
    llm_embedding_layer -> llm_rotary_embedding;
    transformer_positional_encoding -> llm_rotary_embedding [style=dashed, color="#666666"];
    llm_gpt_architecture -> llm_pretraining;
    transformer_transformer_training -> llm_pretraining [style=dashed, color="#666666"];
    llm_pretraining -> llm_next_token_prediction;
    deep_learning_cross_entropy -> llm_next_token_prediction [style=dashed, color="#666666"];
    llm_bert_architecture -> llm_masked_language_modeling;
    llm_pretraining -> llm_masked_language_modeling;
    llm_pretraining -> llm_scaling_laws;
    llm_scaling_laws -> llm_chinchilla_scaling;
    llm_next_token_prediction -> llm_autoregressive_generation;
    transformer_causal_attention -> llm_autoregressive_generation [style=dashed, color="#666666"];
    llm_autoregressive_generation -> llm_sampling_strategies;
    deep_learning_softmax -> llm_sampling_strategies [style=dashed, color="#666666"];
    llm_sampling_strategies -> llm_temperature_sampling;
    llm_temperature_sampling -> llm_top_k_top_p;
    llm_autoregressive_generation -> llm_kv_cache;
    transformer_attention_complexity -> llm_kv_cache [style=dashed, color="#666666"];
    llm_kv_cache -> llm_flash_attention;
    transformer_efficient_attention -> llm_flash_attention [style=dashed, color="#666666"];
    llm_pretraining -> llm_finetuning;
    llm_finetuning -> llm_instruction_tuning;
    llm_instruction_tuning -> llm_chat_format;
    llm_instruction_tuning -> llm_rlhf;
    llm_rlhf -> llm_reward_model;
    llm_reward_model -> llm_ppo;
    llm_rlhf -> llm_dpo;
    llm_finetuning -> llm_peft;
    llm_peft -> llm_lora;
    llm_lora -> llm_qlora;
    llm_quantization -> llm_qlora;
    llm_gpt_architecture -> llm_quantization;
    llm_chat_format -> llm_prompting_techniques;
    llm_prompting_techniques -> llm_few_shot_prompting;
    llm_few_shot_prompting -> llm_chain_of_thought;
    llm_rotary_embedding -> llm_context_length_extension;
    llm_kv_cache -> llm_context_length_extension;
    llm_embedding_layer -> llm_sentence_embeddings;
    llm_bert_architecture -> llm_sentence_embeddings;
    llm_chat_format -> agent_agent_fundamentals [style=dashed, color="#666666"];
    llm_chain_of_thought -> agent_agent_fundamentals [style=dashed, color="#666666"];
    agent_agent_fundamentals -> agent_agent_loop;
    agent_agent_fundamentals -> agent_tool_use;
    agent_tool_use -> agent_function_calling_format;
    agent_function_calling_format -> agent_tool_selection;
    agent_tool_selection -> agent_multi_tool_orchestration;
    agent_planning_strategies -> agent_multi_tool_orchestration;
    agent_tool_use -> agent_code_execution;
    agent_agent_fundamentals -> agent_rag;
    llm_sentence_embeddings -> agent_rag [style=dashed, color="#666666"];
    llm_context_length_extension -> agent_rag [style=dashed, color="#666666"];
    agent_rag -> agent_semantic_search;
    llm_sentence_embeddings -> agent_semantic_search [style=dashed, color="#666666"];
    agent_semantic_search -> agent_vector_database;
    agent_rag -> agent_chunking_strategies;
    agent_semantic_search -> agent_hybrid_search;
    agent_chunking_strategies -> agent_advanced_rag;
    agent_hybrid_search -> agent_advanced_rag;
    agent_advanced_rag -> agent_agentic_rag;
    agent_agent_loop -> agent_agentic_rag;
    agent_agent_fundamentals -> agent_reasoning_techniques;
    llm_chain_of_thought -> agent_reasoning_techniques [style=dashed, color="#666666"];
    agent_reasoning_techniques -> agent_react;
    agent_tool_use -> agent_react;
    agent_reasoning_techniques -> agent_self_reflection;
    agent_self_reflection -> agent_reflexion;
    agent_agent_loop -> agent_reflexion;
    agent_react -> agent_planning_strategies;
    agent_planning_strategies -> agent_hierarchical_planning;
    agent_agent_loop -> agent_agent_memory;
    agent_agent_memory -> agent_short_term_memory;
    agent_agent_memory -> agent_long_term_memory;
    agent_vector_database -> agent_long_term_memory;
    agent_short_term_memory -> agent_memory_management;
    agent_long_term_memory -> agent_memory_management;
    agent_agent_loop -> agent_multi_agent_basics;
    agent_tool_use -> agent_multi_agent_basics;
    agent_multi_agent_basics -> agent_agent_communication;
    agent_multi_agent_basics -> agent_agent_roles;
    agent_agent_communication -> agent_coordination_patterns;
    agent_agent_roles -> agent_coordination_patterns;
    agent_agent_loop -> agent_agent_frameworks;
    agent_tool_use -> agent_agent_frameworks;
    agent_agent_frameworks -> agent_langchain_basics;
    agent_agent_frameworks -> agent_autogen_basics;
    agent_multi_agent_basics -> agent_autogen_basics;
    agent_multi_tool_orchestration -> agent_complex_workflows;
    agent_coordination_patterns -> agent_complex_workflows;
    agent_langchain_basics -> agent_complex_workflows;
    agent_code_execution -> agent_data_analysis_agent;
    agent_rag -> agent_data_analysis_agent;
    agent_agentic_rag -> agent_research_agent;
    agent_planning_strategies -> agent_research_agent;
    agent_complex_workflows -> agent_production_deployment;
    transformer_transformer_encoder -> practical_tools_hf_transformers_intro [style=dashed, color="#666666"];
    transformer_transformer_decoder -> practical_tools_hf_transformers_intro [style=dashed, color="#666666"];
    practical_tools_hf_transformers_intro -> practical_tools_hf_pipeline;
    practical_tools_hf_transformers_intro -> practical_tools_hf_tokenizer;
    llm_tokenization -> practical_tools_hf_tokenizer [style=dashed, color="#666666"];
    llm_bpe -> practical_tools_hf_tokenizer [style=dashed, color="#666666"];
    practical_tools_hf_transformers_intro -> practical_tools_hf_model_loading;
    llm_bert_architecture -> practical_tools_hf_model_loading [style=dashed, color="#666666"];
    llm_gpt_architecture -> practical_tools_hf_model_loading [style=dashed, color="#666666"];
    practical_tools_hf_model_loading -> practical_tools_hf_embeddings;
    practical_tools_hf_tokenizer -> practical_tools_hf_embeddings;
    math_foundations_dot_product -> practical_tools_hf_embeddings [style=dashed, color="#666666"];
    llm_sentence_embeddings -> practical_tools_hf_embeddings [style=dashed, color="#666666"];
    practical_tools_hf_transformers_intro -> practical_tools_hf_datasets;
    practical_tools_hf_datasets -> practical_tools_hf_data_collator;
    practical_tools_hf_tokenizer -> practical_tools_hf_data_collator;
    practical_tools_hf_data_collator -> practical_tools_hf_dataloader;
    deep_learning_mini_batch -> practical_tools_hf_dataloader [style=dashed, color="#666666"];
    practical_tools_hf_model_loading -> practical_tools_hf_training_loop;
    practical_tools_hf_dataloader -> practical_tools_hf_training_loop;
    deep_learning_backpropagation -> practical_tools_hf_training_loop [style=dashed, color="#666666"];
    deep_learning_adamw -> practical_tools_hf_training_loop [style=dashed, color="#666666"];
    deep_learning_gradient_descent -> practical_tools_hf_training_loop [style=dashed, color="#666666"];
    practical_tools_hf_model_loading -> practical_tools_hf_optimizer_scheduler;
    deep_learning_adamw -> practical_tools_hf_optimizer_scheduler [style=dashed, color="#666666"];
    deep_learning_learning_rate -> practical_tools_hf_optimizer_scheduler [style=dashed, color="#666666"];
    practical_tools_hf_training_loop -> practical_tools_hf_evaluation;
    practical_tools_hf_evaluation -> practical_tools_hf_trainer_api;
    practical_tools_hf_training_loop -> practical_tools_hf_accelerate;
    practical_tools_hf_model_loading -> practical_tools_encoder_decoder_comparison;
    llm_bert_architecture -> practical_tools_encoder_decoder_comparison [style=dashed, color="#666666"];
    llm_gpt_architecture -> practical_tools_encoder_decoder_comparison [style=dashed, color="#666666"];
    llm_masked_language_modeling -> practical_tools_encoder_decoder_comparison [style=dashed, color="#666666"];
    llm_next_token_prediction -> practical_tools_encoder_decoder_comparison [style=dashed, color="#666666"];
    practical_tools_encoder_decoder_comparison -> practical_tools_cross_attention_understanding;
    transformer_cross_attention -> practical_tools_cross_attention_understanding [style=dashed, color="#666666"];
    llm_quantization -> practical_tools_ollama_intro [style=dashed, color="#666666"];
    agent_rag -> practical_tools_llamaindex_intro [style=dashed, color="#666666"];
}